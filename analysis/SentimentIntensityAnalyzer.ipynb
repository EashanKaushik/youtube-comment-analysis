{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVtJCV-_YqS1",
    "outputId": "1a7038d4-8b17-4e8a-bbf2-186f0b473ff1"
   },
   "outputs": [],
   "source": [
    "from read_data import combine_all, combine_category, read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iqye9ff7ZCQA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np      \n",
    "import os\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rXagHKk3bS4h"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Njr5cAw9hWzt"
   },
   "outputs": [],
   "source": [
    "df=combine_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1101345 entries, 0 to 60532\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   Name         1101261 non-null  object\n",
      " 1   Comment      1101078 non-null  object\n",
      " 2   Time         1101345 non-null  object\n",
      " 3   Likes        1101345 non-null  int64 \n",
      " 4   Reply Count  1101345 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 50.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Comments from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_q4UgdS_jIBz"
   },
   "outputs": [],
   "source": [
    "df[\"Comment\"]=df[\"Comment\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SaDhaEMsjU0G"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-JI-2_DoS6d",
    "outputId": "c6eb9cad-5463-4600-c4a9-40a56c2ce279"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/srijanmalhotra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/srijanmalhotra/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/srijanmalhotra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/srijanmalhotra/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Lo5Bbh95ote4"
   },
   "outputs": [],
   "source": [
    "df[\"Comment\"] = df[\"Comment\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "BippiwrQseYR",
    "outputId": "69d2d6e8-aa3f-401a-94d6-16a7e1cf8905"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MrBeast</td>\n",
       "      <td>like say video subscribe havenâ€™t already could...</td>\n",
       "      <td>2021-03-27T23:31:32Z</td>\n",
       "      <td>260829</td>\n",
       "      <td>419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alisha Gouker</td>\n",
       "      <td>look scary</td>\n",
       "      <td>2022-04-02T23:34:40Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lea Wodi</td>\n",
       "      <td>work breathe mean time air go</td>\n",
       "      <td>2022-04-02T23:20:34Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex gamer</td>\n",
       "      <td>oxygen</td>\n",
       "      <td>2022-04-02T21:19:16Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deedee Brown</td>\n",
       "      <td>oh know could</td>\n",
       "      <td>2022-04-02T20:43:11Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60528</th>\n",
       "      <td>Poke ax</td>\n",
       "      <td>wow</td>\n",
       "      <td>2021-04-07T18:45:37Z</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60529</th>\n",
       "      <td>Friday game week</td>\n",
       "      <td>first</td>\n",
       "      <td>2021-04-07T18:45:37Z</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60530</th>\n",
       "      <td>Zaptix</td>\n",
       "      <td>first</td>\n",
       "      <td>2021-04-07T18:45:36Z</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60531</th>\n",
       "      <td>ROTNOX</td>\n",
       "      <td>first</td>\n",
       "      <td>2021-04-07T18:45:36Z</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60532</th>\n",
       "      <td>Peyton Johnson</td>\n",
       "      <td>first</td>\n",
       "      <td>2021-04-07T18:45:34Z</td>\n",
       "      <td>294</td>\n",
       "      <td>166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101345 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name  ... compound\n",
       "0               MrBeast  ...   0.7430\n",
       "1         Alisha Gouker  ...  -0.4939\n",
       "2              Lea Wodi  ...   0.0000\n",
       "3            Alex gamer  ...   0.0000\n",
       "4          Deedee Brown  ...   0.0000\n",
       "...                 ...  ...      ...\n",
       "60528           Poke ax  ...   0.5859\n",
       "60529  Friday game week  ...   0.0000\n",
       "60530            Zaptix  ...   0.0000\n",
       "60531            ROTNOX  ...   0.0000\n",
       "60532    Peyton Johnson  ...   0.0000\n",
       "\n",
       "[1101345 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentid = SentimentIntensityAnalyzer()\n",
    "df[\"sentiments\"] = df[\"Comment\"].apply(lambda x:sentid.polarity_scores(x))\n",
    "df = pd.concat([df.drop([\"sentiments\"],axis=1),df[\"sentiments\"].apply(pd.Series)],axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SentimentIntensityAnalyzer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
